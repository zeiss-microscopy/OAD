{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Install czmodel and dependencies (only needed once)\n",
    "#! pip install --upgrade pip\n",
    "#! pip install czmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# this can be used to switch on/off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple TF2 + Keras model for segmentation (to detect cell nuclei)\n",
    "This notebook the entire workflow of training an ANN with [TensorFlow 2](https://www.tensorflow.org/) using the keras API and exporting the trained model to the [czmodel format](https://pypi.org/project/czmodel/) to be ready for use within the [Intellesis](https://www.zeiss.com/microscopy/int/products/microscope-software/zen-intellesis-image-segmentation-by-deep-learning.html) infrastructure.\n",
    "\n",
    "* The trained model is rather simple (for demo purposed) and trained on a small test dataset.\n",
    "* **Therefore, this notebook is meant to be understood as a guide for exporting trained models**\n",
    "* **The notebook does not show how train a model correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# required imports to train a simple TF2 + Keras model for segmentation and package it as CZMODEL\n",
    "# the CZMODEL can be then imported in ZEN and used for segmentation and image analysis workflows\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# those functions are provided by the PyPi package called czmodel (by ZEISS)\n",
    "from czmodel.util.preprocessing import PerImageStandardization, add_preprocessing_layers\n",
    "from czmodel.model_metadata import ModelMetadata, ModelSpec\n",
    "from czmodel import convert_from_model_spec, convert_from_json_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.0.0-69-g765ac8d16e 2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Optional: suppress TF warnings\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "print(tf.version.GIT_VERSION, tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Pipeline\n",
    "This section describes a simple training procedure that creates a trained Keras model.\n",
    "\n",
    "* Therefore, it only represents the custom training procedure\n",
    "* Such procedure will vary from case to case and will contain more sophisticated ways to generate an optimized Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the parameters for loading the training data\n",
    "\n",
    "# place the original *.png images here\n",
    "IMAGES_FOLDER = 'data/nuclei_images/'\n",
    "\n",
    "# place the respective label *.png images here\n",
    "# masks images have one channel (0=background and 1=nucleus)\n",
    "MASKS_FOLDER = 'data/nuclei_masks/'\n",
    "\n",
    "# define the number of channels\n",
    "# this means using a grayscale image with one channel only\n",
    "CHANNELS = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Read the images\n",
    "# This part contains the logic to read pairs of images and label masks for training !\n",
    "\n",
    "# the the sample images\n",
    "sample_images = sorted([os.path.join(IMAGES_FOLDER, f) for f in os.listdir(IMAGES_FOLDER) \n",
    "                        if os.path.isfile(os.path.join(IMAGES_FOLDER, f))])\n",
    "\n",
    "# get the maks\n",
    "sample_masks = sorted([os.path.join(MASKS_FOLDER, f) for f in os.listdir(MASKS_FOLDER) \n",
    "                       if os.path.isfile(os.path.join(MASKS_FOLDER, f))])\n",
    "\n",
    "# load images as numpy arrays\n",
    "images_loaded = np.asarray([tf.image.decode_image(tf.io.read_file(sample_path), channels=CHANNELS).numpy() \n",
    "                            for sample_path in sample_images])\n",
    "\n",
    "# load labels as numpy arrays\n",
    "masks_loaded = np.asarray([tf.one_hot(tf.image.decode_image(tf.io.read_file(sample_path), channels=1)[...,0], depth=2).numpy()\n",
    "                           for sample_path in sample_masks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Remark: For details see [tf.one_hot](https://www.tensorflow.org/api_docs/python/tf/one_hot)\n",
    "\n",
    "`tf.one_hot creates X channels from X labels: 1 => [0.0, 1.0], 0 => [1.0, 0.0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define a simple model\n",
    "This part defines a simple Keras model with two convolutional layers and softmax activation at the output node. It is also possible to add pre.processing layers to the model here.\n",
    "\n",
    "In order to make the model robust to input scaling we standardize each image before training with the PerImageStandardization layer provided by the `czmodel` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define simple Keras model with two convolutional layers and softmax activation at the output node\n",
    "\n",
    "model = tf.keras.models.Sequential([PerImageStandardization(input_shape=(None, None, 1)),\n",
    "                                    tf.keras.layers.Conv2D(16, 3, padding='same'), \n",
    "                                    tf.keras.layers.Conv2D(2, 1, activation='softmax', padding='same')])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fit the model to the loaded data\n",
    "This part fits the model to the loaded data and evaluates it on the training data. In this test example we do not care about an actual evaluation of the model using validation and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 2s 9ms/sample - loss: 0.6817 - categorical_accuracy: 0.4997\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.6339 - categorical_accuracy: 0.8564\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.6017 - categorical_accuracy: 0.8576\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5795 - categorical_accuracy: 0.8582\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5625 - categorical_accuracy: 0.8593\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5469 - categorical_accuracy: 0.8608\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5316 - categorical_accuracy: 0.8628\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.5157 - categorical_accuracy: 0.8654\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.4992 - categorical_accuracy: 0.8683\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 4ms/sample - loss: 0.4819 - categorical_accuracy: 0.8711\n",
      "200/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 2ms/sample - loss: 0.4325 - categorical_accuracy: 0.8728\n",
      "The model achieves 87.28021383285522% accuracy on the training data.\n"
     ]
    }
   ],
   "source": [
    "# define number of training epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(images_loaded, masks_loaded,\n",
    "          batch_size=32,\n",
    "          epochs=num_epochs)\n",
    "\n",
    "# get the loss and acuary values\n",
    "loss, accuracy = model.evaluate(images_loaded, masks_loaded)\n",
    "\n",
    "# show the final accuracy achieved\n",
    "print(\"The model achieves {}% accuracy on the training data.\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a CZModel from the trained Keras model\n",
    "\n",
    "In this section we export the trained model to the CZModel format using the czmodel library and some additional meta data all possible parameter choices are described in the [ANN model specification](https://pypi.org/project/czmodel/) that can be found on the PyPi packager for `czmodel`.\n",
    "\n",
    "### Define Meta-Data\n",
    "\n",
    "We first define the meta-data needed to run the model within the Intellesis infrastructure. The `czmodel` package offers a named tuple `ModelMetadata` that allows to either parse as JSON file as described or to directly specify the parameters as shown below.\n",
    "\n",
    "### Create a Model Specification Object\n",
    "\n",
    "The export functions provided by the `czmodel` package expect a `ModelSpec` tuple that features the Keras model to be exported and the corresponding model metadata.\n",
    "\n",
    "Therefore, we wrap our model and the `model_metadata` instance into a `ModelSpec` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model metadata\n",
    "model_metadata = ModelMetadata.from_params(name='Simple_Nuclei_SegmentationModel', \n",
    "                                           color_handling='ConvertToMonochrome',\n",
    "                                           pixel_type='Gray16',\n",
    "                                           classes=[\"Background\", \"Nucleus\"],\n",
    "                                           border_size=8)\n",
    "\n",
    "\n",
    "# Create a model specification object used for conversion\n",
    "model_spec = ModelSpec(model=model, model_metadata=model_metadata)\n",
    "\n",
    "# Define dimensions - ZEN Intellesis requires fully defined spatial dimensions.\n",
    "# This is the tile size used by the ZEN TilingClient to pass a. image to the segmentation service.\n",
    "\n",
    "# Important: The tile size has to be chosen s.t. inference is possible with the minimum hardware requirements of Intellesis\n",
    "spatial_dims = 1024, 1024  # Optional: Define target spatial dimensions of the model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perform model export into *.czmodel file format\n",
    "\n",
    "The `czmodel` library offers two functions to perform the actual export. \n",
    "\n",
    "* `convert_from_json_spec` allows to provide a JSON file with all information to convert a model in SavedModel format on disk to a `.czmodel` file that can be loaded with ZEN.\n",
    "* `convert_from_model_spec` expects a `ModelSpec` object, an output path and name and optionally target spatial dimensions for the expected input of the exported model. From this information it creates a `.czmodel` file containing the specified model.\n",
    "\n",
    "```python\n",
    "convert_from_model_spec(model_spec=model_spec, \n",
    "                        output_path=folder_to_store_czmodel, \n",
    "                        output_name=name_of_the_model, \n",
    "                        spatial_dims=spatial_dims)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "convert_from_model_spec(model_spec=model_spec, \n",
    "                        output_path='./czmodel_output', \n",
    "                        output_name='simple_nuclei_segmodel', \n",
    "                        spatial_dims=spatial_dims)\n",
    "\n",
    "# In the example above there will be a \"\"./czmodel_output/simple_nuclei_segmodel.czmodel\" file saved on disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remarks\n",
    "The generated .czmodel file can be directly loaded into ZEN Intellesis to perform segmentation tasks with the trained model.\n",
    "If there is already a trained model in SavedModel format present on disk, it can also be converted by providing a meta-data JSON file as described in the [ANN Model Specification](https://pypi.org/project/czmodel/).\n",
    "\n",
    "The following JSON document describes the same meta-data applied in the use case above:\n",
    "\n",
    "```json\n",
    "{\n",
    "\"BorderSize\": 8,\n",
    "\"ColorHandling\": \"ConvertToMonochrome\",\n",
    "\"PixelType\": \"Gray16\",\n",
    "\"Classes\": [\"Background\", \"Nuclei\"],\n",
    "\"ModelPath\": \"saved_tf2_model_output\",\n",
    "}\n",
    "```\n",
    "\n",
    "This information can be copied to a file e.g. in the current working directory `./model_spec.json` that also contains the trained model in SavedModel format e.g. generated by the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# save the trained TF2.SavedModel as a folder structure\n",
    "# The folder + the JSON file can be also used to import the model in ZEN\n",
    "\n",
    "model.save('./saved_tf2_model_output_dims_unset/')\n",
    "add_preprocessing_layers(model, layers=None, spatial_dims=spatial_dims).save('./saved_tf2_model_output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The CZMODEL file (which is essentially a zip file) contains:\n",
    "\n",
    "* **model guid file**: modelid=e47aabbd-8269-439c-b142-78feec2ed2dd\n",
    "\n",
    "\n",
    "* **model file**: modelid=e47aabbd-8269-439c-b142-78feec2ed2dd.model\n",
    "\n",
    "\n",
    "* **model description**: e47aabbd-8269-439c-b142-78feec2ed2dd.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example of a model XML description**\n",
    "\n",
    "<img src=\"./mdpics/model_xml.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To import the newly created model just use the **`Import`** function of the Intellesis Trainable Segmentation module in ZEN.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Select the **`simple_nuclei_segmodel.czmodel`** file and press the **`Open`** button.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the IP-function **`Segmentation`** to segment an image using the imported CZMODEL (containing the trained network).\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model_IPseg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To use the trained model to analyse an image there are two main options\n",
    "\n",
    "1. directly create an Image Analysis Setting based on the model (no class hierarchy, but very simple)\n",
    "2. assign the trained model to s specific class inside a customized image analysis setting (shown below)\n",
    "\n",
    "The crucial step (when not using option 1) is the Select the correct **`Class Segmentation Method`** inside the Image Analysis Wizard.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model_IA1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use the **`Select Model`** function to assign the trained model and the actual **class** (from the trained model) of interest to assign the model / class to the respective object inside the image analysis setting.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model_IA2.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now the trained model will be used to segment the image. The built-in ZEN Tiling Client automatically  to chunk the image and deal with complex dimensions, like Use the **`Scenes`** etc.\n",
    "\n",
    "Additional Post-Processing options, incl. a Minimum Confidence Threshold can be applied to further refine the results.\n",
    "\n",
    "<img src=\"./mdpics/zen_import_model_IA3.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, the model can be loaded into ZEN by using the **Import** function on the **JSON file**. \n",
    "\n",
    "If the model is supposed to be provided to other parties it is usually easier to exchange .czmodel files instead of SavedModel directories with corresponding JSON meta-data files.\n",
    "\n",
    "The `czmodel` library also provides a `convert_from_json_spec` function that accepts the above mentioned JSON file and creates a CZModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This is an additional way how to create a CZMODEL from a saved TF2 model on disk + JSON file.\n",
    "# The currently recommended way to to create the CZMODEL directly by using czmodel.convert_from_model_spec\n",
    "# the path to the TF2.SavedModel folder is defined in the JSON shown above\n",
    "\n",
    "convert_from_json_spec(model_spec_path='model_spec_dims_unset.json',\n",
    "                       output_path='model_from_json',\n",
    "                       output_name = 'simple_nuclei_segmodel_from_json',\n",
    "                       spatial_dims=spatial_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* the path to the saved model folder is defined in the JSON shown above\n",
    "\n",
    "* **Remark: Due a TF 2.1 bug reloading a model does currently not work correctly.** See issue: https://github.com/tensorflow/tensorflow/issues/37158. This works with TF 2.0 and will be fixed again with TF 2.2. We currently do not have any information if there will be released a patch for TF 2.1 that fixes the issue there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Use the commands below from a terminal to present the notebook as a slideshow.\n",
    "\n",
    "`\n",
    "jupyter nbconvert train_simple_TF2_segmentation_model.ipynb --to slides --post serve \n",
    "    --SlidesExporter.reveal_theme=serif \n",
    "    --SlidesExporter.reveal_scroll=True \n",
    "    --SlidesExporter.reveal_transition=none\n",
    "`\n",
    "\n",
    "Or install the [RISE Extension](https://rise.readthedocs.io/en/stable/) to display a a slideshow directly from within the notebook"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}